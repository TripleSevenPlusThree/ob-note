# 目的

- 什么是机器学习？
	- 让机器（计算机）从大量的数据中学习规律，从而能够对新的数据进行预测或分类等操作。
	- 一种通用的算法模型。
	- 将事物量化
- 为什么要学习机器学习？
	- 于我而言，机器学习就像是数据建模，建立一种通用的算法模型，可以用来解决某种问题或创造价值。
	- 了解一个算法如何诞生，如何从原理，到建立自动化的模型（程序），训练，优化，最后到应用的过程。
	- 除此之外，它与数据分析有关，是其的有力工具。
- 什么是学习？（机器的学习）
	- 拿训练数据调整好的参数与真实值做比较，得到误差，下一次调整参数，让误差更小一点，是一个渐进过程
	- 一步一步的实践，得到反馈做出调整，直到趋近答案

# 基本理论

![[PixPin_2024-12-08_15-55-49.png]]
# 有监督学习

- 使用标记（labeled）的数据进行训练。标记数据是指数据集中的每个样本都有一个对应的目标值或类别标签。
	- 标记即数据属于的类别或是目标值，比如房子大小50平米（特征值），售价100w（目标值）或者 训练集中的被验证过的正确数据，例如要训练一个辨别小狗的模型，那么标记数据就是很多小狗的照片，训练数据（特征值）就是随意的一张照片，小猫的，狮子的，老虎的。

## 线性回归

### 误差项的定义

1. 真实值=预测值+误差
![[PixPin_2024-11-30_19-14-56.png]]
2. 同分布：即训练的数据都来源于同一个分布，如正态分布。
	- 判断方法：
		- 对于离散型随机变量，同分布意味着它们具有相同的分布律，即每个取值对应的概率相等.
		- 对于连续型随机变量，同分布则表示它们具有相同的概率密度函数和分布函数.

### 似然函数


![[PixPin_2024-11-30_19-28-18.png]]

- 利用误差符合高斯分布，将误差转换为权重的公式
- 似然函数， 联合概率最大，因为有很多权重，要使其同时出现的概率的最大（因为符合权重都符合定理，所以出现的概率最大，也就代表着结果最好）
- 因为数据独立，所以同时出现的概率为每个权重的概率的乘积。
![[PixPin_2024-12-01_21-47-59.png]]
![[PixPin_2024-12-01_21-55-17.png]]
![[PixPin_2024-12-01_22-06-17.png]]
- 这种相当于直接求解，求出权重，但是问题在于没有优化迭代，因为只求解一次相当于只用了一个样本特征值，误差较大，所以不可取，并且直接求解不一定可解。
- 需要用优化的思想进行求解

### 梯度下降

![[PixPin_2024-12-01_22-16-17.png]]
![[PixPin_2024-12-01_22-16-03.png]]
![[PixPin_2024-12-01_22-23-04.png]]
![[PixPin_2024-12-01_22-36-45.png]]
- 损失值计算方式，均方误差：
![[PixPin_2024-12-07_14-54-44.png]]
![[PixPin_2024-12-05_16-03-00.png]]
![[PixPin_2024-12-01_22-40-46.png]]
![[PixPin_2024-12-02_20-31-05.png]]

- 学习的过程就在优化迭代中体现：为找到然损失函数最小的参数点，以及以最快的方式找到，必须沿着梯度方向（梯度方向是下降最快的方向），以及每一次下降的步长要小，这样才能让值更加准确（类似于微分）
- 梯度下降，我的理解就是利用导函数，找到导函数为0的点，让极值点变化，当沿着某个方向变化到导函数值为0时，再迭代的话其自变量就收敛于一点了

### 非线性回归

1. 关键在于数据的维度处理，即数据变换
### 模型评估

1. 确保数据独立，之间无联系
2. 交叉验证——将训练集分成n份，每一份都当一次验证集
3. 混淆矩阵——评估分类模型性能

### 模型实验

1. 多种策略的效果对比——梯度下降策略
2. 数据样本量多训练的影响
3. 均方误差看模型拟合效果
4. 岭回归——正则化避免过拟合
	- 加上一个正则化表达式，收缩均方误差，防止过拟合
	- ![[PixPin_2024-12-05_21-22-28.png]]
	- ![[PixPin_2024-12-05_21-21-56.png]]

### 总结

1. 解决一个什么问题？
2. 有什么应用场景？
3. 解决方案是什么？
	1. 原理探究
	2. 证明推导
	3. 模型建立
	4. 如何训练？
		1. 有几种策略？
		2. 效果如何？
4. 需要什么数据？
5. 如何量化？
6. 对数据做什么样的预处理？
7. 编写自动化程序
8. 模型效果实验
9. 模型评估
10. 实际落地应用

- 怎么找到想要的权重值？
	- 转换为以权重theta为自变量的函数
	- 通过迭代逼近的方法找到最适合的theta——这里体现为梯度下降
## 逻辑回归

### 定义

- 经典的二分类算法
- 为什么叫回归？
	- 将类别（真实值）转换为0或1（只有两个类别）
	- 回归的方式找到预测值
	- 映射到sigmoid函数，落在[0,1]区间
	- 本质依然是预测，只不过只有两个真实值，0或1,即找到最适合的theta去预测其是否接近1或0
	- 但是应用为分类，

### 推导
![[PixPin_2024-12-06_18-58-36.png]]
![[PixPin_2024-12-06_19-05-45.png]]
![[PixPin_2024-12-06_19-09-15.png]]
![[PixPin_2024-12-06_19-11-42.png]]
![[PixPin_2024-12-06_19-13-42.png]]
### 总结

- 二分类——每一次回归预测后都映射到sigmod函数，变成概率值。比较两个概率值的大小，谁大取谁，就是那个类别
- 也可以做多分类问题，即应用多次二分类

## 决策树
### 定义
![[PixPin_2024-12-08_19-14-22.png]]
![[PixPin_2024-12-08_19-34-07.png]]
![[PixPin_2024-12-08_19-37-21.png]]
### 决策树构造实例
![[PixPin_2024-12-08_20-35-16.png]]
![[PixPin_2024-12-08_20-39-53.png]]
![[PixPin_2024-12-08_20-41-44.png]]
![[PixPin_2024-12-08_20-45-06.png]]
![[PixPin_2024-12-08_19-54-47.png]]
![[PixPin_2024-12-08_19-59-39.png]]
### 防止过拟合策略——剪枝

![[PixPin_2024-12-08_21-00-32.png]]
![[PixPin_2024-12-08_21-01-14.png]]

### 总结

- 决策树既可以做分类，也可以做回归
	- 分类，叶子节点类别的标签值（如果有多个，则取次数最多的类别）
	- 回归，取叶子节点中所有标签值的平均值
- 如果特征值全是不同的数值怎么处理？
	- 离散化：转换为多个区间
	- 使用C4.5或CART算法，即使用不同的方法作为不同的分类标准
- 个人觉得决策树更适合做难以量化的非数值数据的分类问题。
# 无监督学习

- 使用未标记（unlabeled）的数据进行训练的机器学习方法。在无监督学习中，数据集中没有给定明确的目标值或类别标签。
## K-means
### 定义

- 聚类，将相似的数据分成一类

![[PixPin_2024-12-10_10-41-55.png]]

### 优化流程

![[PixPin_2024-12-10_10-56-30.png]]
![[PixPin_2024-12-10_11-15-41.png]]

### DBSCAN 聚类算法

#### 定义


 
## 贝叶斯